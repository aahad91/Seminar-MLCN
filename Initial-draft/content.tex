%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START ADDING TEXT HERE
%
% Feel free to use \include commands to structure text in smaller
% pieces
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Abstract gives a brief summary of the main points of a paper:
\begin{abstract}
\textbf{Abstract - }
\end{abstract}

% the actual content, usually separated over a number of sections
% each section is assigned a label, in order to be able to put a
% crossreference to it

\section{Introduction}
\label{sec:introduction}
In recent years with the evolution of technology, Internet of Things (IoT) devices are increasing day by day. According to Ericsson mobility report\cite{}, there will be 17\% (approx. 22.3 billion) increase in IoT devices by 2024. Functionally IoT is defined as \emph{"The Internet of Things allows people and things to be connected Anytime, Anyplace, with Anything and Anyone"} \cite{European commission 2008}. IoT devices have served mankind in many ways such as from smart houses to smart cities, smart transportation systems and many medical applications. These IoT applications enables many devices connected to network and generates alot of heterogeneous data also known as BigData which requires special data processing models and Infrastructure support. Processing BigData required alot of resources and cloud computing theoratically provides it unlimited resources\cite{fog-comp-survey}. But there is downside of using cloud computing for such complex computation as it is more costly when it comes to computation power, storage and bandwidth. Computation need to be performed at the node level and only the aggregated data need to send to central node for further computations and analysis. This de-centralized approach will save alot of computation power as well as bandwidth requriments\cite{fog-comp-survey}. To overcome the downside of cloud computing, the terminology fog computing is used. Fog computing allows the computation at the egde of network instead of central core. \par
Fog computing is defined as \emph{"an archietecture that uses one or a collaborative multitude of end-user clients or near-user edge devices to carry out a substantial amount of storage (rather than stored primarily in cloud data centers), communication (rather than routed over the internet backbone), and control, configuration, measurement and management (rather than controlled primarily by network gateways such as those in the LTE (telecommunication) core)‚Äù}\cite{Chiang 2015; Aazam and Huh 2014}. Traditionally, user applications running in cloud access the cloud core network through access points for data exchange to fetch data from data-centers \cite{Bittencourt2017}. In fog computing these access points also serves as resource providers such as computation power and storage etc. and are called "cloudlets"\cite{Bittencourt2017}. Figure \ref{fig:fog-arch} show the top-level archietecture of the fog computing. \par
\begin{figure}
  \includegraphics[width=70mm]{figures/mlcn-fog-1.pdf}
  \caption{Fog computing: Top-level overview\cite{Bittencourt2017}}
  \label{fig:fog-arch}
\end{figure}
Fog computing is responsible for providing resources to IoT devices for processing. Traditionally these resources are allocated as VMs from different cloud infrastructures such as AWS, Google, OpenStack, etc. to run the applications. VMs are considered resource greedy and require more computational resources. Alternate is to use the Containers such as Docker which are light-weight, requires less resources and based on micro-service architecture. Large applications are split into containers based on the main processes of the application. This increasing number of containers per application required the proper monitoring for health check and resource consumption. The most commonly used orchestrator for containers is Kubernetes. \par
Kubernetes act as IaaS for fog computing to provide resource for IoT applications. Kubernetes is an open-source platform for management, deployment and scaling of containers. In Kubernetes, applications are deployed as pod consisting of multiple containers. When the configuration of deploying application is passed to Kubernetes, it checks for the availability of resources and deploys afterward. Kubernetes default resource scheduler monitor and deploys the pod using computation power-based scheduling mechanism and does not consider latency and available bandwidth, which is considered important while dealing with data-centric application. Example of data-centric application is weather forecast that receives data from scattered IoT devices and provide prediction. If the data is lost or delayed due higher latency and poor bandwidth, timely decisions cannot be made that leads to disaster. To overcome this drawback of Kubernetes, author proposed an alternate Kubernetes scheduler that consider network resources along with computational resources.
\section{Background}
\label{sec:backgroud}
This section explains about the Kubernetes main components, working as Orchestrator and built-in resource provisioning techniques.
\subsection{Kubernetes Main Components}
\label{sec:k8s_main_comp}
\begin{figure}
  \includegraphics[width=70mm]{figures/mlcn-k8s-components.pdf}
  \caption{Kubernetes Cluster: Main Components\cite{Santos2019}}
  \label{fig:k8s-comp}
\end{figure}
Kubernetes is an open-source project that manages the container-based applications deployed over multiple hosts. Kubernetes act as Orchestrator which is responsible for deploying, managing, scaling of container-based application\cite{kubernetes-github-repo}. Figure \ref{fig:k8s-comp} shows the main components coupled to work as one unit called kubernetes. It is based on master-slave model, consisting of one \emph{master node} and multiple \emph{worker nodes} \cite{Santos2019}. \emph{worker nodes} can be either physical or virtual resource such as physical servers or virtual machines. \emph{master node} communicates with \emph{worker nodes} using \emph{API} calls. \emph{API server} uses RESTFul API, for managing all the \emph{API} calls is also part of \emph{master node}. End-users communicates with kubernetes cluster using \emph{Kubectl}, which forward user requests to \emph{API server} and intern gets the result. \emph{Etcd} stores the data as key-value pair,which is used to store all configurations, states. It is one of the main component of kubernetes, which maintians the state across the cluster for synchorization of data. \emph{Control Manager} is resposible for monitoring of \emph{Etcd}. For any state change of cluster, \emph{Control Manager} forward the new state request using \emph{API server}. \emph{Kube Scheduler} is discussed later in section \ref{sec:k8s_scheduler}. On \emph{worker node}, node agent known as \emph{Kubelet} which is resposible for maintain state based on \emph{API server} request. For any state change communicated by \emph{API server}, \emph{Kubelet} performs the desired operation such as starting or deleting of Docker containers. \emph{Image Registry} is resposible for managing the images required to create the container applications. \emph{Pod} is main component of \emph{worker node} where all the applications are deployed. Single \emph{Pod} represents the application which consists of multiple containers based on the services of application. \emph{Pod} is the collections of containers, volumes in an isolated environment which means there is no cross communication between two \emph{Pods}. Containers running in a \emph{Pod} share the same IP Address \cite{Santos2019}. Containers communicates using different ports, hence there is a limitaion to this apporach as two containers listening on same port cannot be in same \emph{Pod}\cite{Santos2019}.
\subsection{Kubernetes as Orchestrator}
\label{sec:k8s_orchestrator}
Orchestrator is responsible for automating the processes that requires alot of human effort. As discussed in \cite{containerjournal}, Orchestrator is responsible for following:
\begin{itemize}
  \item Starting or stopping of different applications.
  \item Ensure Scalabilty of application for high usage demands.
  \item Management of load across different nodes to avoid resource overhead.
  \item Monitoring health of applications.
\end{itemize}
There are many Orchestrator currently available, but the most widely used are OpenStack and Kubernetes.
\begin{enumerate}
  \item OpenStack Orchestration: It provides template-based orcestration for cloud application to run on OpenStack. Template allows to create resources such as Virtual machines(Instances), Storage (Volumes), Networks etc. These resources are coupled together as OpenStack \emph{Project} to run cloud application\cite{openstackOrchestrator}.
  \item Kubernetes Orchestration: It is responsible for automating deployment, scaling and management of container-based applications. \emph{master node} orcestrates the application across various \emph{worker node} based on resource availability.
\end{enumerate}
\subsection{Kubernetes Resource Provisioning}
\label{sec:k8s_scheduler}
\begin{figure}
  \includegraphics[width=70mm]{figures/mlcn-k8s-scheduler.pdf}
  \caption{Kubernetes Scheduler: Working\cite{Santos2019}}
  \label{fig:k8s-sch}
\end{figure}
When the user provides the configuration for creating new \emph{pod} using \emph{Kubectl}. \emph{Pod} is added to the waiting queue with all the other \emph{pods}. \emph{Kube-Scheduler} which is the default scheduler of kubernetes, decides which \emph{pod} deploys on which \emph{worker node} based on some criteria. Figure \ref{fig:k8s-sch} shows the default scheduling mechanism where \emph{pod} is deployed by passing through following steps: \emph{node filtering} and \emph{node priority or scoring}\cite{Santos2019}. In the kubernetes cluster\emph{worker nods} meeting the requirement of \emph{ped} are called \emph{feasible nodes}\cite{k8s}.
\subsubsection{\emph{Node Filtering}}
\label{sec:node-filter}
The first step of deploying \emph{pod} is \emph{node filtering} in which \emph{Kube-Scheduler} will select the \emph{feasible nodes} based on the \emph{pod} configuration by applying some filters. These filter are also called \emph{predicates}. Following is the list of \emph{predicates} that are supported by \emph{Kube-Scheduler}\cite{k8s}:
\begin{enumerate}
  \item \textbf{PodFitsHostPorts:} This filters checks the \emph{worker node} for the ports requested by the \emph{pod}.
  \item \textbf{PodFitHost:} This filter checks for \emph{worker node} with hostname mentioned in \emph{pod} configuration.
  \item \textbf{PodFitsResources:} This filter checks for the available resources i.e. CPUs and Memory to run the \emph{pod}.
  \item \textbf{NoDiskConflict:} This filter checks the \emph{worker node} for the volumes requested by the \emph{pod} and are already mounted.
  \item \textbf{CheckNodeMemoryPressure:} This filter checks the \emph{worker node} for over-utilization of Memory.
  \item \textbf{CheckNodeDiskPressure:} This filter checks the \emph{worker node} disk space and filesystem, sufficient to run the \emph{pod}.
  \item \textbf{CheckNodeCondition:} This filter checks the \emph{worker node} for available disk space, networking configuration and that of \emph{Kubelet} is reachable or not.
  \item \textbf{PodMatchNodeSelector:} This filter search for the \emph{worker node} based on the label mentioned in \emph{pod} configuration. These labels allows the user to deploy the \emph{pod} on specfic \emph{worker node}(node-affinity)\cite{Santos2019}. Other usecase of using label is to restrict the \emph{pod} deployment based on other \emph{pod} already deployed on that \emph{worker node} (pod-anti-affinity) \cite{Santos2019}. These affinity rules are based on \emph{Tolerations} and \emph{Taints} which are defined as key-value pair along with their effects. \emph{Tolerations} are defined in \emph{pod} configration whereas \emph{Taints} are set for \emph{worker node}. Both \emph{Tolerations} and \emph{Taints} work together to ensure \emph{pod} is not deployed on inapproriate \emph{worker node} \cite{k8s}.
\end{enumerate}
Using the above mentioned filters(\emph{predicates}), \emph{Kube-Scheduler} returns the \emph{feasible node} for \emph{pod} deployment. If no \emph{feasible node} is found, \emph{pod} remains un-scheduled and error message is generated for failed deployment \cite{Santos2019}. If the list of \emph{feasible node} is returned as the result of applying filters then \emph{Kube-Scheduler} moves to second step \emph{node priority or scoring}.
\subsubsection{\emph{Node Priority/Scoring}}
\label{sec:node-priority}
\emph{Kube-Scheduler} assign rank to each \emph{worker node} that passes the \emph{node filtering} stage. These ranks/priorities sort the list of \emph{worker node} based on best-fit for \emph{pod} deployment. These priorities are set based on following criteria\cite{k8s}:
\begin{enumerate}
  \item \textbf{SelectorSpreadPriority:} "This priority algorithm tries to minimize the number of deployed pods belonging to the same service on the same node or on the same zone/rack"\cite{Santos2019}.
  \item \textbf{InterPodAffinityPriority:} This priority sets the score for \emph{worker node} based on the pod-affinity rule mentioned above.
  \item \textbf{LeastRequestedPriority:} This priority sets the score for \emph{worker node} based on the higher available resources i.e. CPU and Memory.
  \item \textbf{MostRequestedPriority:} This priority sets the score for \emph{worker node} based on the minimum resource requirement for \emph{pod} deployment.
  \item \textbf{RequestedToCapacityRatioPriority:} This priority sets the score for \emph{worker node} based on request to capacity using ResourceAllocationPriority.
  \item \textbf{BalancedResourceAllocation:} This priority selects the \emph{worker node} with balanced resource utilization.
  \item \textbf{NodeAffinityPriority:} This priority selects the \emph{worker node} based on node-affinity rule. \emph{Worker node} with the required label will be given priority.
  \item \textbf{TaintTolerationPriority:} This priority sets the score for \emph{worker node} based on their \emph{taints} with respect to \emph{tolerations} mentioned in \emph{pod} configuration\cite{Santos2019}.
  \item \textbf{ImageLocalityPriority:} This priority sets the score for \emph{worker node} based on the availability of the image on \emph{worker node} required to build the containers for \emph{pod}.
  \item \textbf{EqualPriority:} This priority sets the equal weight to all the \emph{worker nodes}.
\end{enumerate}
The default \emph{Kube-Scheduler} works efficiently for the resources such as CPU, Memory and storage but does not consider the networking resource which is consider critical resource in many use-case scenarios. Considering one application of Fog Computing such as IoT based smart cities which is data sensitive use-case. Ensuring that no data is lost, networking resource need to be configured properly. Default \emph{Kube-Scheduler} does not check the network latency and available bandwidth for \emph{worker node}. In order to cater this drawback, author in \cite{Santos2019} proposed a scheduler thats checks for the network resources along with the dedault \emph{Kube-Scheduler}.
\section{Kubernetes Network-based Resource Provisioning}
\label{sec: k8s_ns}
\begin{itemize}
  \item write about why we need network-based resource provisioning
  \item main factors consideration (e.g bandwidth and latency)
  \item workflow of network-based scheduler
\end{itemize}

\section{Performance Evaluation}
\label{sec:Performance_eval}
\begin{itemize}
  \item Write about the considered use-case of Fog Computing for Evaluation
\end{itemize}
\subsection{Expermentation Setup}
\label{sec:setup}
\begin{itemize}
  \item setup of Kubernetes base on the mentioned use-case of Fog Computing with diagram
\end{itemize}

\subsection{Analysis of Kubernetes Default and Network-based Resource Provisioning}
\label{sec:analysis}
\begin{itemize}
  \item write about the Performance difference between default Kubernetes scheduler and network based scheduler with supporting result tables and graphs
\end{itemize}

\section{Comparison of Network-based Resource Provisioning Solutions}
\label{sec:related_work}
\begin{itemize}
  \item Compare different solutions based on the following criteria:
\end{itemize}

\subsection{Orchestrator}
\label{sec:infra}
\begin{itemize}
  \item write about the differences between Kubernetes(main-paper)\cite{Santos2019} and other available cloud solutions such as Fogernetes\cite{Wobker2018} and \cite{Reale}.
\end{itemize}

\subsection{Resource Provisioning Techniques}
\begin{itemize}
  \item difference between different resource scheduling techniques such  as \cite{Bittencourt2017}, \cite{Haja2019} etc.
\end{itemize}

\section{Conclusion}
\label{sec:concl}

\section{Further Research Topics}
\label{sec:research}
\begin{itemize}
  \item after writing the seminar, if there is any improvement that can be done, will be added in this section.
\end{itemize}
